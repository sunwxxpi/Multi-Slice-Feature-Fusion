=======================================================================================================================================
Layer (type (var_name):depth-idx)                       Input Shape          Kernel Shape         Output Shape         Param %
=======================================================================================================================================
Unet (Unet)                                             [16, 1, 512, 512]    --                   [16, 5, 512, 512]         --
├─MixVisionTransformerEncoder (encoder): 1-1            [16, 1, 512, 512]    --                   [16, 1, 512, 512]         --
│    └─OverlapPatchEmbed (patch_embed1): 2-1            [16, 1, 512, 512]    --                   [16, 64, 128, 128]        --
│    │    └─Conv2d (proj): 3-1                          [16, 1, 512, 512]    [7, 7]               [16, 64, 128, 128]     0.01%
│    │    └─LayerNorm (norm): 3-2                       [16, 64, 128, 128]   --                   [16, 64, 128, 128]     0.00%
│    └─Sequential (block1): 2-2                         [16, 64, 128, 128]   --                   [16, 64, 128, 128]        --
│    │    └─Block (0): 3-3                              [16, 64, 128, 128]   --                   [16, 64, 128, 128]        --
│    │    │    └─LayerNorm (norm1): 4-1                 [16, 16384, 64]      --                   [16, 16384, 64]        0.00%
│    │    │    └─Attention (attn): 4-2                  [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    │    └─Linear (q): 5-1                   [16, 16384, 64]      --                   [16, 16384, 64]        0.02%
│    │    │    │    └─Conv2d (sr): 5-2                  [16, 64, 128, 128]   [8, 8]               [16, 64, 16, 16]       0.95%
│    │    │    │    └─LayerNorm (norm): 5-3             [16, 256, 64]        --                   [16, 256, 64]          0.00%
│    │    │    │    └─Linear (kv): 5-4                  [16, 256, 64]        --                   [16, 256, 128]         0.03%
│    │    │    │    └─Dropout (attn_drop): 5-5          [16, 1, 16384, 256]  --                   [16, 1, 16384, 256]       --
│    │    │    │    └─Linear (proj): 5-6                [16, 16384, 64]      --                   [16, 16384, 64]        0.02%
│    │    │    │    └─Dropout (proj_drop): 5-7          [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    └─Identity (drop_path): 4-3              [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    └─LayerNorm (norm2): 4-4                 [16, 16384, 64]      --                   [16, 16384, 64]        0.00%
│    │    │    └─Mlp (mlp): 4-5                         [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    │    └─Linear (fc1): 5-8                 [16, 16384, 64]      --                   [16, 16384, 256]       0.06%
│    │    │    │    └─DWConv (dwconv): 5-9              [16, 16384, 256]     --                   [16, 16384, 256]       0.01%
│    │    │    │    └─GELU (act): 5-10                  [16, 16384, 256]     --                   [16, 16384, 256]          --
│    │    │    │    └─Dropout (drop): 5-11              [16, 16384, 256]     --                   [16, 16384, 256]          --
│    │    │    │    └─Linear (fc2): 5-12                [16, 16384, 256]     --                   [16, 16384, 64]        0.06%
│    │    │    │    └─Dropout (drop): 5-13              [16, 16384, 64]      --                   [16, 16384, 64]      (recursive)
│    │    │    └─Identity (drop_path): 4-6              [16, 16384, 64]      --                   [16, 16384, 64]      (recursive)
│    │    └─Block (1): 3-4                              [16, 64, 128, 128]   --                   [16, 64, 128, 128]        --
│    │    │    └─LayerNorm (norm1): 4-7                 [16, 16384, 64]      --                   [16, 16384, 64]        0.00%
│    │    │    └─Attention (attn): 4-8                  [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    │    └─Linear (q): 5-14                  [16, 16384, 64]      --                   [16, 16384, 64]        0.02%
│    │    │    │    └─Conv2d (sr): 5-15                 [16, 64, 128, 128]   [8, 8]               [16, 64, 16, 16]       0.95%
│    │    │    │    └─LayerNorm (norm): 5-16            [16, 256, 64]        --                   [16, 256, 64]          0.00%
│    │    │    │    └─Linear (kv): 5-17                 [16, 256, 64]        --                   [16, 256, 128]         0.03%
│    │    │    │    └─Dropout (attn_drop): 5-18         [16, 1, 16384, 256]  --                   [16, 1, 16384, 256]       --
│    │    │    │    └─Linear (proj): 5-19               [16, 16384, 64]      --                   [16, 16384, 64]        0.02%
│    │    │    │    └─Dropout (proj_drop): 5-20         [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    └─DropPath (drop_path): 4-9              [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    └─LayerNorm (norm2): 4-10                [16, 16384, 64]      --                   [16, 16384, 64]        0.00%
│    │    │    └─Mlp (mlp): 4-11                        [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    │    └─Linear (fc1): 5-21                [16, 16384, 64]      --                   [16, 16384, 256]       0.06%
│    │    │    │    └─DWConv (dwconv): 5-22             [16, 16384, 256]     --                   [16, 16384, 256]       0.01%
│    │    │    │    └─GELU (act): 5-23                  [16, 16384, 256]     --                   [16, 16384, 256]          --
│    │    │    │    └─Dropout (drop): 5-24              [16, 16384, 256]     --                   [16, 16384, 256]          --
│    │    │    │    └─Linear (fc2): 5-25                [16, 16384, 256]     --                   [16, 16384, 64]        0.06%
│    │    │    │    └─Dropout (drop): 5-26              [16, 16384, 64]      --                   [16, 16384, 64]      (recursive)
│    │    │    └─DropPath (drop_path): 4-12             [16, 16384, 64]      --                   [16, 16384, 64]      (recursive)
│    │    └─Block (2): 3-5                              [16, 64, 128, 128]   --                   [16, 64, 128, 128]        --
│    │    │    └─LayerNorm (norm1): 4-13                [16, 16384, 64]      --                   [16, 16384, 64]        0.00%
│    │    │    └─Attention (attn): 4-14                 [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    │    └─Linear (q): 5-27                  [16, 16384, 64]      --                   [16, 16384, 64]        0.02%
│    │    │    │    └─Conv2d (sr): 5-28                 [16, 64, 128, 128]   [8, 8]               [16, 64, 16, 16]       0.95%
│    │    │    │    └─LayerNorm (norm): 5-29            [16, 256, 64]        --                   [16, 256, 64]          0.00%
│    │    │    │    └─Linear (kv): 5-30                 [16, 256, 64]        --                   [16, 256, 128]         0.03%
│    │    │    │    └─Dropout (attn_drop): 5-31         [16, 1, 16384, 256]  --                   [16, 1, 16384, 256]       --
│    │    │    │    └─Linear (proj): 5-32               [16, 16384, 64]      --                   [16, 16384, 64]        0.02%
│    │    │    │    └─Dropout (proj_drop): 5-33         [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    └─DropPath (drop_path): 4-15             [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    └─LayerNorm (norm2): 4-16                [16, 16384, 64]      --                   [16, 16384, 64]        0.00%
│    │    │    └─Mlp (mlp): 4-17                        [16, 16384, 64]      --                   [16, 16384, 64]           --
│    │    │    │    └─Linear (fc1): 5-34                [16, 16384, 64]      --                   [16, 16384, 256]       0.06%
│    │    │    │    └─DWConv (dwconv): 5-35             [16, 16384, 256]     --                   [16, 16384, 256]       0.01%
│    │    │    │    └─GELU (act): 5-36                  [16, 16384, 256]     --                   [16, 16384, 256]          --
│    │    │    │    └─Dropout (drop): 5-37              [16, 16384, 256]     --                   [16, 16384, 256]          --
│    │    │    │    └─Linear (fc2): 5-38                [16, 16384, 256]     --                   [16, 16384, 64]        0.06%
│    │    │    │    └─Dropout (drop): 5-39              [16, 16384, 64]      --                   [16, 16384, 64]      (recursive)
│    │    │    └─DropPath (drop_path): 4-18             [16, 16384, 64]      --                   [16, 16384, 64]      (recursive)
│    └─LayerNorm (norm1): 2-3                           [16, 64, 128, 128]   --                   [16, 64, 128, 128]     0.00%
│    └─OverlapPatchEmbed (patch_embed2): 2-4            [16, 64, 128, 128]   --                   [16, 128, 64, 64]         --
│    │    └─Conv2d (proj): 3-6                          [16, 64, 128, 128]   [3, 3]               [16, 128, 64, 64]      0.27%
│    │    └─LayerNorm (norm): 3-7                       [16, 128, 64, 64]    --                   [16, 128, 64, 64]      0.00%
│    └─Sequential (block2): 2-5                         [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    └─Block (0): 3-8                              [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    │    └─LayerNorm (norm1): 4-19                [16, 4096, 128]      --                   [16, 4096, 128]        0.00%
│    │    │    └─Attention (attn): 4-20                 [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    │    └─Linear (q): 5-40                  [16, 4096, 128]      --                   [16, 4096, 128]        0.06%
│    │    │    │    └─Conv2d (sr): 5-41                 [16, 128, 64, 64]    [4, 4]               [16, 128, 16, 16]      0.95%
│    │    │    │    └─LayerNorm (norm): 5-42            [16, 256, 128]       --                   [16, 256, 128]         0.00%
│    │    │    │    └─Linear (kv): 5-43                 [16, 256, 128]       --                   [16, 256, 256]         0.12%
│    │    │    │    └─Dropout (attn_drop): 5-44         [16, 2, 4096, 256]   --                   [16, 2, 4096, 256]        --
│    │    │    │    └─Linear (proj): 5-45               [16, 4096, 128]      --                   [16, 4096, 128]        0.06%
│    │    │    │    └─Dropout (proj_drop): 5-46         [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    └─DropPath (drop_path): 4-21             [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    └─LayerNorm (norm2): 4-22                [16, 4096, 128]      --                   [16, 4096, 128]        0.00%
│    │    │    └─Mlp (mlp): 4-23                        [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    │    └─Linear (fc1): 5-47                [16, 4096, 128]      --                   [16, 4096, 512]        0.24%
│    │    │    │    └─DWConv (dwconv): 5-48             [16, 4096, 512]      --                   [16, 4096, 512]        0.02%
│    │    │    │    └─GELU (act): 5-49                  [16, 4096, 512]      --                   [16, 4096, 512]           --
│    │    │    │    └─Dropout (drop): 5-50              [16, 4096, 512]      --                   [16, 4096, 512]           --
│    │    │    │    └─Linear (fc2): 5-51                [16, 4096, 512]      --                   [16, 4096, 128]        0.24%
│    │    │    │    └─Dropout (drop): 5-52              [16, 4096, 128]      --                   [16, 4096, 128]      (recursive)
│    │    │    └─DropPath (drop_path): 4-24             [16, 4096, 128]      --                   [16, 4096, 128]      (recursive)
│    │    └─Block (1): 3-9                              [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    │    └─LayerNorm (norm1): 4-25                [16, 4096, 128]      --                   [16, 4096, 128]        0.00%
│    │    │    └─Attention (attn): 4-26                 [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    │    └─Linear (q): 5-53                  [16, 4096, 128]      --                   [16, 4096, 128]        0.06%
│    │    │    │    └─Conv2d (sr): 5-54                 [16, 128, 64, 64]    [4, 4]               [16, 128, 16, 16]      0.95%
│    │    │    │    └─LayerNorm (norm): 5-55            [16, 256, 128]       --                   [16, 256, 128]         0.00%
│    │    │    │    └─Linear (kv): 5-56                 [16, 256, 128]       --                   [16, 256, 256]         0.12%
│    │    │    │    └─Dropout (attn_drop): 5-57         [16, 2, 4096, 256]   --                   [16, 2, 4096, 256]        --
│    │    │    │    └─Linear (proj): 5-58               [16, 4096, 128]      --                   [16, 4096, 128]        0.06%
│    │    │    │    └─Dropout (proj_drop): 5-59         [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    └─DropPath (drop_path): 4-27             [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    └─LayerNorm (norm2): 4-28                [16, 4096, 128]      --                   [16, 4096, 128]        0.00%
│    │    │    └─Mlp (mlp): 4-29                        [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    │    └─Linear (fc1): 5-60                [16, 4096, 128]      --                   [16, 4096, 512]        0.24%
│    │    │    │    └─DWConv (dwconv): 5-61             [16, 4096, 512]      --                   [16, 4096, 512]        0.02%
│    │    │    │    └─GELU (act): 5-62                  [16, 4096, 512]      --                   [16, 4096, 512]           --
│    │    │    │    └─Dropout (drop): 5-63              [16, 4096, 512]      --                   [16, 4096, 512]           --
│    │    │    │    └─Linear (fc2): 5-64                [16, 4096, 512]      --                   [16, 4096, 128]        0.24%
│    │    │    │    └─Dropout (drop): 5-65              [16, 4096, 128]      --                   [16, 4096, 128]      (recursive)
│    │    │    └─DropPath (drop_path): 4-30             [16, 4096, 128]      --                   [16, 4096, 128]      (recursive)
│    │    └─Block (2): 3-10                             [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    │    └─LayerNorm (norm1): 4-31                [16, 4096, 128]      --                   [16, 4096, 128]        0.00%
│    │    │    └─Attention (attn): 4-32                 [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    │    └─Linear (q): 5-66                  [16, 4096, 128]      --                   [16, 4096, 128]        0.06%
│    │    │    │    └─Conv2d (sr): 5-67                 [16, 128, 64, 64]    [4, 4]               [16, 128, 16, 16]      0.95%
│    │    │    │    └─LayerNorm (norm): 5-68            [16, 256, 128]       --                   [16, 256, 128]         0.00%
│    │    │    │    └─Linear (kv): 5-69                 [16, 256, 128]       --                   [16, 256, 256]         0.12%
│    │    │    │    └─Dropout (attn_drop): 5-70         [16, 2, 4096, 256]   --                   [16, 2, 4096, 256]        --
│    │    │    │    └─Linear (proj): 5-71               [16, 4096, 128]      --                   [16, 4096, 128]        0.06%
│    │    │    │    └─Dropout (proj_drop): 5-72         [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    └─DropPath (drop_path): 4-33             [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    └─LayerNorm (norm2): 4-34                [16, 4096, 128]      --                   [16, 4096, 128]        0.00%
│    │    │    └─Mlp (mlp): 4-35                        [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    │    └─Linear (fc1): 5-73                [16, 4096, 128]      --                   [16, 4096, 512]        0.24%
│    │    │    │    └─DWConv (dwconv): 5-74             [16, 4096, 512]      --                   [16, 4096, 512]        0.02%
│    │    │    │    └─GELU (act): 5-75                  [16, 4096, 512]      --                   [16, 4096, 512]           --
│    │    │    │    └─Dropout (drop): 5-76              [16, 4096, 512]      --                   [16, 4096, 512]           --
│    │    │    │    └─Linear (fc2): 5-77                [16, 4096, 512]      --                   [16, 4096, 128]        0.24%
│    │    │    │    └─Dropout (drop): 5-78              [16, 4096, 128]      --                   [16, 4096, 128]      (recursive)
│    │    │    └─DropPath (drop_path): 4-36             [16, 4096, 128]      --                   [16, 4096, 128]      (recursive)
│    │    └─Block (3): 3-11                             [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    │    └─LayerNorm (norm1): 4-37                [16, 4096, 128]      --                   [16, 4096, 128]        0.00%
│    │    │    └─Attention (attn): 4-38                 [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    │    └─Linear (q): 5-79                  [16, 4096, 128]      --                   [16, 4096, 128]        0.06%
│    │    │    │    └─Conv2d (sr): 5-80                 [16, 128, 64, 64]    [4, 4]               [16, 128, 16, 16]      0.95%
│    │    │    │    └─LayerNorm (norm): 5-81            [16, 256, 128]       --                   [16, 256, 128]         0.00%
│    │    │    │    └─Linear (kv): 5-82                 [16, 256, 128]       --                   [16, 256, 256]         0.12%
│    │    │    │    └─Dropout (attn_drop): 5-83         [16, 2, 4096, 256]   --                   [16, 2, 4096, 256]        --
│    │    │    │    └─Linear (proj): 5-84               [16, 4096, 128]      --                   [16, 4096, 128]        0.06%
│    │    │    │    └─Dropout (proj_drop): 5-85         [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    └─DropPath (drop_path): 4-39             [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    └─LayerNorm (norm2): 4-40                [16, 4096, 128]      --                   [16, 4096, 128]        0.00%
│    │    │    └─Mlp (mlp): 4-41                        [16, 4096, 128]      --                   [16, 4096, 128]           --
│    │    │    │    └─Linear (fc1): 5-86                [16, 4096, 128]      --                   [16, 4096, 512]        0.24%
│    │    │    │    └─DWConv (dwconv): 5-87             [16, 4096, 512]      --                   [16, 4096, 512]        0.02%
│    │    │    │    └─GELU (act): 5-88                  [16, 4096, 512]      --                   [16, 4096, 512]           --
│    │    │    │    └─Dropout (drop): 5-89              [16, 4096, 512]      --                   [16, 4096, 512]           --
│    │    │    │    └─Linear (fc2): 5-90                [16, 4096, 512]      --                   [16, 4096, 128]        0.24%
│    │    │    │    └─Dropout (drop): 5-91              [16, 4096, 128]      --                   [16, 4096, 128]      (recursive)
│    │    │    └─DropPath (drop_path): 4-42             [16, 4096, 128]      --                   [16, 4096, 128]      (recursive)
│    └─LayerNorm (norm2): 2-6                           [16, 128, 64, 64]    --                   [16, 128, 64, 64]      0.00%
│    └─OverlapPatchEmbed (patch_embed3): 2-7            [16, 128, 64, 64]    --                   [16, 320, 32, 32]         --
│    │    └─Conv2d (proj): 3-12                         [16, 128, 64, 64]    [3, 3]               [16, 320, 32, 32]      1.34%
│    │    └─LayerNorm (norm): 3-13                      [16, 320, 32, 32]    --                   [16, 320, 32, 32]      0.00%
│    └─Sequential (block3): 2-8                         [16, 320, 32, 32]    --                   [16, 320, 32, 32]         --
│    │    └─Block (0): 3-14                             [16, 320, 32, 32]    --                   [16, 320, 32, 32]         --
│    │    │    └─LayerNorm (norm1): 4-43                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Attention (attn): 4-44                 [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (q): 5-92                  [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Conv2d (sr): 5-93                 [16, 320, 32, 32]    [2, 2]               [16, 320, 16, 16]      1.49%
│    │    │    │    └─LayerNorm (norm): 5-94            [16, 256, 320]       --                   [16, 256, 320]         0.00%
│    │    │    │    └─Linear (kv): 5-95                 [16, 256, 320]       --                   [16, 256, 640]         0.75%
│    │    │    │    └─Dropout (attn_drop): 5-96         [16, 5, 1024, 256]   --                   [16, 5, 1024, 256]        --
│    │    │    │    └─Linear (proj): 5-97               [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Dropout (proj_drop): 5-98         [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─DropPath (drop_path): 4-45             [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─LayerNorm (norm2): 4-46                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Mlp (mlp): 4-47                        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (fc1): 5-99                [16, 1024, 320]      --                   [16, 1024, 1280]       1.50%
│    │    │    │    └─DWConv (dwconv): 5-100            [16, 1024, 1280]     --                   [16, 1024, 1280]       0.05%
│    │    │    │    └─GELU (act): 5-101                 [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Dropout (drop): 5-102             [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Linear (fc2): 5-103               [16, 1024, 1280]     --                   [16, 1024, 320]        1.49%
│    │    │    │    └─Dropout (drop): 5-104             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    │    └─DropPath (drop_path): 4-48             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    └─Block (1): 3-15                             [16, 320, 32, 32]    --                   [16, 320, 32, 32]         --
│    │    │    └─LayerNorm (norm1): 4-49                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Attention (attn): 4-50                 [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (q): 5-105                 [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Conv2d (sr): 5-106                [16, 320, 32, 32]    [2, 2]               [16, 320, 16, 16]      1.49%
│    │    │    │    └─LayerNorm (norm): 5-107           [16, 256, 320]       --                   [16, 256, 320]         0.00%
│    │    │    │    └─Linear (kv): 5-108                [16, 256, 320]       --                   [16, 256, 640]         0.75%
│    │    │    │    └─Dropout (attn_drop): 5-109        [16, 5, 1024, 256]   --                   [16, 5, 1024, 256]        --
│    │    │    │    └─Linear (proj): 5-110              [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Dropout (proj_drop): 5-111        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─DropPath (drop_path): 4-51             [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─LayerNorm (norm2): 4-52                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Mlp (mlp): 4-53                        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (fc1): 5-112               [16, 1024, 320]      --                   [16, 1024, 1280]       1.50%
│    │    │    │    └─DWConv (dwconv): 5-113            [16, 1024, 1280]     --                   [16, 1024, 1280]       0.05%
│    │    │    │    └─GELU (act): 5-114                 [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Dropout (drop): 5-115             [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Linear (fc2): 5-116               [16, 1024, 1280]     --                   [16, 1024, 320]        1.49%
│    │    │    │    └─Dropout (drop): 5-117             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    │    └─DropPath (drop_path): 4-54             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    └─Block (2): 3-16                             [16, 320, 32, 32]    --                   [16, 320, 32, 32]         --
│    │    │    └─LayerNorm (norm1): 4-55                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Attention (attn): 4-56                 [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (q): 5-118                 [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Conv2d (sr): 5-119                [16, 320, 32, 32]    [2, 2]               [16, 320, 16, 16]      1.49%
│    │    │    │    └─LayerNorm (norm): 5-120           [16, 256, 320]       --                   [16, 256, 320]         0.00%
│    │    │    │    └─Linear (kv): 5-121                [16, 256, 320]       --                   [16, 256, 640]         0.75%
│    │    │    │    └─Dropout (attn_drop): 5-122        [16, 5, 1024, 256]   --                   [16, 5, 1024, 256]        --
│    │    │    │    └─Linear (proj): 5-123              [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Dropout (proj_drop): 5-124        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─DropPath (drop_path): 4-57             [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─LayerNorm (norm2): 4-58                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Mlp (mlp): 4-59                        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (fc1): 5-125               [16, 1024, 320]      --                   [16, 1024, 1280]       1.50%
│    │    │    │    └─DWConv (dwconv): 5-126            [16, 1024, 1280]     --                   [16, 1024, 1280]       0.05%
│    │    │    │    └─GELU (act): 5-127                 [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Dropout (drop): 5-128             [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Linear (fc2): 5-129               [16, 1024, 1280]     --                   [16, 1024, 320]        1.49%
│    │    │    │    └─Dropout (drop): 5-130             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    │    └─DropPath (drop_path): 4-60             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    └─Block (3): 3-17                             [16, 320, 32, 32]    --                   [16, 320, 32, 32]         --
│    │    │    └─LayerNorm (norm1): 4-61                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Attention (attn): 4-62                 [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (q): 5-131                 [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Conv2d (sr): 5-132                [16, 320, 32, 32]    [2, 2]               [16, 320, 16, 16]      1.49%
│    │    │    │    └─LayerNorm (norm): 5-133           [16, 256, 320]       --                   [16, 256, 320]         0.00%
│    │    │    │    └─Linear (kv): 5-134                [16, 256, 320]       --                   [16, 256, 640]         0.75%
│    │    │    │    └─Dropout (attn_drop): 5-135        [16, 5, 1024, 256]   --                   [16, 5, 1024, 256]        --
│    │    │    │    └─Linear (proj): 5-136              [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Dropout (proj_drop): 5-137        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─DropPath (drop_path): 4-63             [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─LayerNorm (norm2): 4-64                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Mlp (mlp): 4-65                        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (fc1): 5-138               [16, 1024, 320]      --                   [16, 1024, 1280]       1.50%
│    │    │    │    └─DWConv (dwconv): 5-139            [16, 1024, 1280]     --                   [16, 1024, 1280]       0.05%
│    │    │    │    └─GELU (act): 5-140                 [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Dropout (drop): 5-141             [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Linear (fc2): 5-142               [16, 1024, 1280]     --                   [16, 1024, 320]        1.49%
│    │    │    │    └─Dropout (drop): 5-143             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    │    └─DropPath (drop_path): 4-66             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    └─Block (4): 3-18                             [16, 320, 32, 32]    --                   [16, 320, 32, 32]         --
│    │    │    └─LayerNorm (norm1): 4-67                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Attention (attn): 4-68                 [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (q): 5-144                 [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Conv2d (sr): 5-145                [16, 320, 32, 32]    [2, 2]               [16, 320, 16, 16]      1.49%
│    │    │    │    └─LayerNorm (norm): 5-146           [16, 256, 320]       --                   [16, 256, 320]         0.00%
│    │    │    │    └─Linear (kv): 5-147                [16, 256, 320]       --                   [16, 256, 640]         0.75%
│    │    │    │    └─Dropout (attn_drop): 5-148        [16, 5, 1024, 256]   --                   [16, 5, 1024, 256]        --
│    │    │    │    └─Linear (proj): 5-149              [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Dropout (proj_drop): 5-150        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─DropPath (drop_path): 4-69             [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─LayerNorm (norm2): 4-70                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Mlp (mlp): 4-71                        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (fc1): 5-151               [16, 1024, 320]      --                   [16, 1024, 1280]       1.50%
│    │    │    │    └─DWConv (dwconv): 5-152            [16, 1024, 1280]     --                   [16, 1024, 1280]       0.05%
│    │    │    │    └─GELU (act): 5-153                 [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Dropout (drop): 5-154             [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Linear (fc2): 5-155               [16, 1024, 1280]     --                   [16, 1024, 320]        1.49%
│    │    │    │    └─Dropout (drop): 5-156             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    │    └─DropPath (drop_path): 4-72             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    └─Block (5): 3-19                             [16, 320, 32, 32]    --                   [16, 320, 32, 32]         --
│    │    │    └─LayerNorm (norm1): 4-73                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Attention (attn): 4-74                 [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (q): 5-157                 [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Conv2d (sr): 5-158                [16, 320, 32, 32]    [2, 2]               [16, 320, 16, 16]      1.49%
│    │    │    │    └─LayerNorm (norm): 5-159           [16, 256, 320]       --                   [16, 256, 320]         0.00%
│    │    │    │    └─Linear (kv): 5-160                [16, 256, 320]       --                   [16, 256, 640]         0.75%
│    │    │    │    └─Dropout (attn_drop): 5-161        [16, 5, 1024, 256]   --                   [16, 5, 1024, 256]        --
│    │    │    │    └─Linear (proj): 5-162              [16, 1024, 320]      --                   [16, 1024, 320]        0.37%
│    │    │    │    └─Dropout (proj_drop): 5-163        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─DropPath (drop_path): 4-75             [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    └─LayerNorm (norm2): 4-76                [16, 1024, 320]      --                   [16, 1024, 320]        0.00%
│    │    │    └─Mlp (mlp): 4-77                        [16, 1024, 320]      --                   [16, 1024, 320]           --
│    │    │    │    └─Linear (fc1): 5-164               [16, 1024, 320]      --                   [16, 1024, 1280]       1.50%
│    │    │    │    └─DWConv (dwconv): 5-165            [16, 1024, 1280]     --                   [16, 1024, 1280]       0.05%
│    │    │    │    └─GELU (act): 5-166                 [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Dropout (drop): 5-167             [16, 1024, 1280]     --                   [16, 1024, 1280]          --
│    │    │    │    └─Linear (fc2): 5-168               [16, 1024, 1280]     --                   [16, 1024, 320]        1.49%
│    │    │    │    └─Dropout (drop): 5-169             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    │    │    └─DropPath (drop_path): 4-78             [16, 1024, 320]      --                   [16, 1024, 320]      (recursive)
│    └─LayerNorm (norm3): 2-9                           [16, 320, 32, 32]    --                   [16, 320, 32, 32]      0.00%
│    └─OverlapPatchEmbed (patch_embed4): 2-10           [16, 320, 32, 32]    --                   [16, 512, 16, 16]         --
│    │    └─Conv2d (proj): 3-20                         [16, 320, 32, 32]    [3, 3]               [16, 512, 16, 16]      5.37%
│    │    └─LayerNorm (norm): 3-21                      [16, 512, 16, 16]    --                   [16, 512, 16, 16]      0.00%
│    └─Sequential (block4): 2-11                        [16, 512, 16, 16]    --                   [16, 512, 16, 16]         --
│    │    └─Block (0): 3-22                             [16, 512, 16, 16]    --                   [16, 512, 16, 16]         --
│    │    │    └─LayerNorm (norm1): 4-79                [16, 256, 512]       --                   [16, 256, 512]         0.00%
│    │    │    └─Attention (attn): 4-80                 [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    │    └─Linear (q): 5-170                 [16, 256, 512]       --                   [16, 256, 512]         0.96%
│    │    │    │    └─Linear (kv): 5-171                [16, 256, 512]       --                   [16, 256, 1024]        1.91%
│    │    │    │    └─Dropout (attn_drop): 5-172        [16, 8, 256, 256]    --                   [16, 8, 256, 256]         --
│    │    │    │    └─Linear (proj): 5-173              [16, 256, 512]       --                   [16, 256, 512]         0.96%
│    │    │    │    └─Dropout (proj_drop): 5-174        [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    └─DropPath (drop_path): 4-81             [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    └─LayerNorm (norm2): 4-82                [16, 256, 512]       --                   [16, 256, 512]         0.00%
│    │    │    └─Mlp (mlp): 4-83                        [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    │    └─Linear (fc1): 5-175               [16, 256, 512]       --                   [16, 256, 2048]        3.82%
│    │    │    │    └─DWConv (dwconv): 5-176            [16, 256, 2048]      --                   [16, 256, 2048]        0.07%
│    │    │    │    └─GELU (act): 5-177                 [16, 256, 2048]      --                   [16, 256, 2048]           --
│    │    │    │    └─Dropout (drop): 5-178             [16, 256, 2048]      --                   [16, 256, 2048]           --
│    │    │    │    └─Linear (fc2): 5-179               [16, 256, 2048]      --                   [16, 256, 512]         3.82%
│    │    │    │    └─Dropout (drop): 5-180             [16, 256, 512]       --                   [16, 256, 512]       (recursive)
│    │    │    └─DropPath (drop_path): 4-84             [16, 256, 512]       --                   [16, 256, 512]       (recursive)
│    │    └─Block (1): 3-23                             [16, 512, 16, 16]    --                   [16, 512, 16, 16]         --
│    │    │    └─LayerNorm (norm1): 4-85                [16, 256, 512]       --                   [16, 256, 512]         0.00%
│    │    │    └─Attention (attn): 4-86                 [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    │    └─Linear (q): 5-181                 [16, 256, 512]       --                   [16, 256, 512]         0.96%
│    │    │    │    └─Linear (kv): 5-182                [16, 256, 512]       --                   [16, 256, 1024]        1.91%
│    │    │    │    └─Dropout (attn_drop): 5-183        [16, 8, 256, 256]    --                   [16, 8, 256, 256]         --
│    │    │    │    └─Linear (proj): 5-184              [16, 256, 512]       --                   [16, 256, 512]         0.96%
│    │    │    │    └─Dropout (proj_drop): 5-185        [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    └─DropPath (drop_path): 4-87             [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    └─LayerNorm (norm2): 4-88                [16, 256, 512]       --                   [16, 256, 512]         0.00%
│    │    │    └─Mlp (mlp): 4-89                        [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    │    └─Linear (fc1): 5-186               [16, 256, 512]       --                   [16, 256, 2048]        3.82%
│    │    │    │    └─DWConv (dwconv): 5-187            [16, 256, 2048]      --                   [16, 256, 2048]        0.07%
│    │    │    │    └─GELU (act): 5-188                 [16, 256, 2048]      --                   [16, 256, 2048]           --
│    │    │    │    └─Dropout (drop): 5-189             [16, 256, 2048]      --                   [16, 256, 2048]           --
│    │    │    │    └─Linear (fc2): 5-190               [16, 256, 2048]      --                   [16, 256, 512]         3.82%
│    │    │    │    └─Dropout (drop): 5-191             [16, 256, 512]       --                   [16, 256, 512]       (recursive)
│    │    │    └─DropPath (drop_path): 4-90             [16, 256, 512]       --                   [16, 256, 512]       (recursive)
│    │    └─Block (2): 3-24                             [16, 512, 16, 16]    --                   [16, 512, 16, 16]         --
│    │    │    └─LayerNorm (norm1): 4-91                [16, 256, 512]       --                   [16, 256, 512]         0.00%
│    │    │    └─Attention (attn): 4-92                 [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    │    └─Linear (q): 5-192                 [16, 256, 512]       --                   [16, 256, 512]         0.96%
│    │    │    │    └─Linear (kv): 5-193                [16, 256, 512]       --                   [16, 256, 1024]        1.91%
│    │    │    │    └─Dropout (attn_drop): 5-194        [16, 8, 256, 256]    --                   [16, 8, 256, 256]         --
│    │    │    │    └─Linear (proj): 5-195              [16, 256, 512]       --                   [16, 256, 512]         0.96%
│    │    │    │    └─Dropout (proj_drop): 5-196        [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    └─DropPath (drop_path): 4-93             [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    └─LayerNorm (norm2): 4-94                [16, 256, 512]       --                   [16, 256, 512]         0.00%
│    │    │    └─Mlp (mlp): 4-95                        [16, 256, 512]       --                   [16, 256, 512]            --
│    │    │    │    └─Linear (fc1): 5-197               [16, 256, 512]       --                   [16, 256, 2048]        3.82%
│    │    │    │    └─DWConv (dwconv): 5-198            [16, 256, 2048]      --                   [16, 256, 2048]        0.07%
│    │    │    │    └─GELU (act): 5-199                 [16, 256, 2048]      --                   [16, 256, 2048]           --
│    │    │    │    └─Dropout (drop): 5-200             [16, 256, 2048]      --                   [16, 256, 2048]           --
│    │    │    │    └─Linear (fc2): 5-201               [16, 256, 2048]      --                   [16, 256, 512]         3.82%
│    │    │    │    └─Dropout (drop): 5-202             [16, 256, 512]       --                   [16, 256, 512]       (recursive)
│    │    │    └─DropPath (drop_path): 4-96             [16, 256, 512]       --                   [16, 256, 512]       (recursive)
│    └─LayerNorm (norm4): 2-12                          [16, 512, 16, 16]    --                   [16, 512, 16, 16]      0.00%
├─UnetDecoder (decoder): 1-2                            [16, 1, 512, 512]    --                   [16, 16, 512, 512]        --
│    └─Identity (center): 2-13                          [16, 512, 16, 16]    --                   [16, 512, 16, 16]         --
│    └─ModuleList (blocks): 2-14                        --                   --                   --                        --
│    │    └─DecoderBlock (0): 3-25                      [16, 512, 16, 16]    --                   [16, 256, 32, 32]         --
│    │    │    └─Attention (attention1): 4-97           [16, 832, 32, 32]    --                   [16, 832, 32, 32]         --
│    │    │    │    └─Identity (attention): 5-203       [16, 832, 32, 32]    --                   [16, 832, 32, 32]         --
│    │    │    └─Conv2dReLU (conv1): 4-98               [16, 832, 32, 32]    --                   [16, 256, 32, 32]         --
│    │    │    │    └─Conv2d (0): 5-204                 [16, 832, 32, 32]    [3, 3]               [16, 256, 32, 32]      6.98%
│    │    │    │    └─BatchNorm2d (1): 5-205            [16, 256, 32, 32]    --                   [16, 256, 32, 32]      0.00%
│    │    │    │    └─ReLU (2): 5-206                   [16, 256, 32, 32]    --                   [16, 256, 32, 32]         --
│    │    │    └─Conv2dReLU (conv2): 4-99               [16, 256, 32, 32]    --                   [16, 256, 32, 32]         --
│    │    │    │    └─Conv2d (0): 5-207                 [16, 256, 32, 32]    [3, 3]               [16, 256, 32, 32]      2.15%
│    │    │    │    └─BatchNorm2d (1): 5-208            [16, 256, 32, 32]    --                   [16, 256, 32, 32]      0.00%
│    │    │    │    └─ReLU (2): 5-209                   [16, 256, 32, 32]    --                   [16, 256, 32, 32]         --
│    │    │    └─Attention (attention2): 4-100          [16, 256, 32, 32]    --                   [16, 256, 32, 32]         --
│    │    │    │    └─Identity (attention): 5-210       [16, 256, 32, 32]    --                   [16, 256, 32, 32]         --
│    │    └─DecoderBlock (1): 3-26                      [16, 256, 32, 32]    --                   [16, 128, 64, 64]         --
│    │    │    └─Attention (attention1): 4-101          [16, 384, 64, 64]    --                   [16, 384, 64, 64]         --
│    │    │    │    └─Identity (attention): 5-211       [16, 384, 64, 64]    --                   [16, 384, 64, 64]         --
│    │    │    └─Conv2dReLU (conv1): 4-102              [16, 384, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    │    │    └─Conv2d (0): 5-212                 [16, 384, 64, 64]    [3, 3]               [16, 128, 64, 64]      1.61%
│    │    │    │    └─BatchNorm2d (1): 5-213            [16, 128, 64, 64]    --                   [16, 128, 64, 64]      0.00%
│    │    │    │    └─ReLU (2): 5-214                   [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    │    └─Conv2dReLU (conv2): 4-103              [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    │    │    └─Conv2d (0): 5-215                 [16, 128, 64, 64]    [3, 3]               [16, 128, 64, 64]      0.54%
│    │    │    │    └─BatchNorm2d (1): 5-216            [16, 128, 64, 64]    --                   [16, 128, 64, 64]      0.00%
│    │    │    │    └─ReLU (2): 5-217                   [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    │    └─Attention (attention2): 4-104          [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    │    │    └─Identity (attention): 5-218       [16, 128, 64, 64]    --                   [16, 128, 64, 64]         --
│    │    └─DecoderBlock (2): 3-27                      [16, 128, 64, 64]    --                   [16, 64, 128, 128]        --
│    │    │    └─Attention (attention1): 4-105          [16, 192, 128, 128]  --                   [16, 192, 128, 128]       --
│    │    │    │    └─Identity (attention): 5-219       [16, 192, 128, 128]  --                   [16, 192, 128, 128]       --
│    │    │    └─Conv2dReLU (conv1): 4-106              [16, 192, 128, 128]  --                   [16, 64, 128, 128]        --
│    │    │    │    └─Conv2d (0): 5-220                 [16, 192, 128, 128]  [3, 3]               [16, 64, 128, 128]     0.40%
│    │    │    │    └─BatchNorm2d (1): 5-221            [16, 64, 128, 128]   --                   [16, 64, 128, 128]     0.00%
│    │    │    │    └─ReLU (2): 5-222                   [16, 64, 128, 128]   --                   [16, 64, 128, 128]        --
│    │    │    └─Conv2dReLU (conv2): 4-107              [16, 64, 128, 128]   --                   [16, 64, 128, 128]        --
│    │    │    │    └─Conv2d (0): 5-223                 [16, 64, 128, 128]   [3, 3]               [16, 64, 128, 128]     0.13%
│    │    │    │    └─BatchNorm2d (1): 5-224            [16, 64, 128, 128]   --                   [16, 64, 128, 128]     0.00%
│    │    │    │    └─ReLU (2): 5-225                   [16, 64, 128, 128]   --                   [16, 64, 128, 128]        --
│    │    │    └─Attention (attention2): 4-108          [16, 64, 128, 128]   --                   [16, 64, 128, 128]        --
│    │    │    │    └─Identity (attention): 5-226       [16, 64, 128, 128]   --                   [16, 64, 128, 128]        --
│    │    └─DecoderBlock (3): 3-28                      [16, 64, 128, 128]   --                   [16, 32, 256, 256]        --
│    │    │    └─Attention (attention1): 4-109          [16, 64, 256, 256]   --                   [16, 64, 256, 256]        --
│    │    │    │    └─Identity (attention): 5-227       [16, 64, 256, 256]   --                   [16, 64, 256, 256]        --
│    │    │    └─Conv2dReLU (conv1): 4-110              [16, 64, 256, 256]   --                   [16, 32, 256, 256]        --
│    │    │    │    └─Conv2d (0): 5-228                 [16, 64, 256, 256]   [3, 3]               [16, 32, 256, 256]     0.07%
│    │    │    │    └─BatchNorm2d (1): 5-229            [16, 32, 256, 256]   --                   [16, 32, 256, 256]     0.00%
│    │    │    │    └─ReLU (2): 5-230                   [16, 32, 256, 256]   --                   [16, 32, 256, 256]        --
│    │    │    └─Conv2dReLU (conv2): 4-111              [16, 32, 256, 256]   --                   [16, 32, 256, 256]        --
│    │    │    │    └─Conv2d (0): 5-231                 [16, 32, 256, 256]   [3, 3]               [16, 32, 256, 256]     0.03%
│    │    │    │    └─BatchNorm2d (1): 5-232            [16, 32, 256, 256]   --                   [16, 32, 256, 256]     0.00%
│    │    │    │    └─ReLU (2): 5-233                   [16, 32, 256, 256]   --                   [16, 32, 256, 256]        --
│    │    │    └─Attention (attention2): 4-112          [16, 32, 256, 256]   --                   [16, 32, 256, 256]        --
│    │    │    │    └─Identity (attention): 5-234       [16, 32, 256, 256]   --                   [16, 32, 256, 256]        --
│    │    └─DecoderBlock (4): 3-29                      [16, 32, 256, 256]   --                   [16, 16, 512, 512]        --
│    │    │    └─Conv2dReLU (conv1): 4-113              [16, 32, 512, 512]   --                   [16, 16, 512, 512]        --
│    │    │    │    └─Conv2d (0): 5-235                 [16, 32, 512, 512]   [3, 3]               [16, 16, 512, 512]     0.02%
│    │    │    │    └─BatchNorm2d (1): 5-236            [16, 16, 512, 512]   --                   [16, 16, 512, 512]     0.00%
│    │    │    │    └─ReLU (2): 5-237                   [16, 16, 512, 512]   --                   [16, 16, 512, 512]        --
│    │    │    └─Conv2dReLU (conv2): 4-114              [16, 16, 512, 512]   --                   [16, 16, 512, 512]        --
│    │    │    │    └─Conv2d (0): 5-238                 [16, 16, 512, 512]   [3, 3]               [16, 16, 512, 512]     0.01%
│    │    │    │    └─BatchNorm2d (1): 5-239            [16, 16, 512, 512]   --                   [16, 16, 512, 512]     0.00%
│    │    │    │    └─ReLU (2): 5-240                   [16, 16, 512, 512]   --                   [16, 16, 512, 512]        --
│    │    │    └─Attention (attention2): 4-115          [16, 16, 512, 512]   --                   [16, 16, 512, 512]        --
│    │    │    │    └─Identity (attention): 5-241       [16, 16, 512, 512]   --                   [16, 16, 512, 512]        --
├─SegmentationHead (segmentation_head): 1-3             [16, 16, 512, 512]   --                   [16, 5, 512, 512]         --
│    └─Conv2d (0): 2-15                                 [16, 16, 512, 512]   [3, 3]               [16, 5, 512, 512]      0.00%
│    └─Identity (1): 2-16                               [16, 5, 512, 512]    --                   [16, 5, 512, 512]         --
│    └─Activation (2): 2-17                             [16, 5, 512, 512]    --                   [16, 5, 512, 512]         --
│    │    └─Identity (activation): 3-30                 [16, 5, 512, 512]    --                   [16, 5, 512, 512]         --
=======================================================================================================================================
Total params: 27,471,317
Trainable params: 27,471,317
Non-trainable params: 0
Total mult-adds (G): 219.91
=======================================================================================================================================
Input size (MB): 16.78
Forward/backward pass size (MB): 18203.28
Params size (MB): 109.89
Estimated Total Size (MB): 18329.94
=======================================================================================================================================